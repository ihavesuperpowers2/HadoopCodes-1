{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "Stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    text_returned = re.sub(regex, '', text)\n",
    "    return text_returned\n",
    "\n",
    "def finding_all_unique_words_and_freq(words):\n",
    "    words_unique = []\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        if word not in words_unique:\n",
    "            words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "        word_freq[word] = words.count(word)\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\a_v.txt\n",
      "As you all must be aware, to fight against the spread of COVID19, our Honourable Prime Minister Narendra Modi has announced #JanataCurfew (social distancing by staying in your house and not going out anywhere) on 22nd March (Sunday).\n",
      " \n",
      "Analytics Vidhya whole-heartedly supports this initiative and invites every data science aspirant and professional to stay at home and participate in the Janata Hack - a knowledge competition on Machine Learning & Data Science on 22nd March 2020 from 0700 hrs to 2100 hrs IST in support of the Janata Curfew.\n",
      "The Coronavirus pandemic is going to affect businesses across the world. Large companies are likely to suffer loss in revenue, which would eventually lead to job cuts. However, the large IT firms have good news for their employees; many of these companies are pledging no layoffs for 2020.\n",
      "\n",
      "Salesforce CEO recently pledged no significant layoffs for the next 90 days. On his Twitter account, Salesforce CEO Marc Benioff published a statement. He clarified that there will be no significant layoffs in the next three months. He also urged the senior leadership to do their part by helping workers keep their jobs.\n",
      "['must', 'aware', 'fight', 'spread', 'covid', 'honourable', 'prime', 'minister', 'narendra', 'modi', 'announced', 'janatacurfew', 'social', 'distancing', 'staying', 'house', 'going', 'anywhere', 'nd', 'march', 'sunday', 'analytics', 'vidhya', 'wholeheartedly', 'supports', 'initiative', 'invites', 'every', 'data', 'science', 'aspirant', 'professional', 'stay', 'home', 'participate', 'janata', 'hack', 'knowledge', 'competition', 'machine', 'learning', 'data', 'science', 'nd', 'march', 'hrs', 'hrs', 'ist', 'support', 'janata', 'curfew', 'coronavirus', 'pandemic', 'going', 'affect', 'businesses', 'across', 'world', 'large', 'companies', 'likely', 'suffer', 'loss', 'revenue', 'would', 'eventually', 'lead', 'job', 'cuts', 'however', 'large', 'firms', 'good', 'news', 'employees', 'many', 'companies', 'pledging', 'layoffs', 'salesforce', 'ceo', 'recently', 'pledged', 'significant', 'layoffs', 'next', 'days', 'twitter', 'account', 'salesforce', 'ceo', 'marc', 'benioff', 'published', 'statement', 'clarified', 'significant', 'layoffs', 'next', 'three', 'months', 'also', 'urged', 'senior', 'leadership', 'part', 'helping', 'workers', 'keep', 'jobs']\n",
      "data\\talentspirit.txt\n",
      "Are you also at home due to the COVID-19 pandemic? We created a personal discount for you, so you can spend your time learning new, higher-paying job skills.\n",
      "\n",
      "You can apply the discount on any of our Nanodegree programs at udac?ity.c?om. Jump into learning new skills t?oday!\n",
      "\n",
      "Please note you'll need to sign in with, or create a Udacity account in order to access this offer.\n",
      "\n",
      "The premier engineering institute, Indian Institute of Technology (IIT) Kanpur has invited applications for a certification course on Python. The online course can be accessed for free until July 31, 2020.\n",
      "\n",
      "Coronavirus pandemic has forced universities and colleges to close their physical classes. IIT Kanpur is offering an online course on Python Programming through an online portal. Students can enrol for the course and access it for free till July 31st. You can even opt for the certification by paying a nominal fee by taking an online examination.\n",
      "\n",
      "The Python Programming courses covers all major aspects of the programming language. It is designed by the faculty member of Computer Science and Engineering, Prof Amey Karkare. The curriculum is designed for beginners, assuming that the student does not have any prior programming knowledge or experience.\n",
      "\n",
      "The course covers over 8 hours of online lectures. Since these are pre-recorded session, students can access these lectures as per their convenience. The syllabus also includes source code files for hands-on practice. Students can use this reference code to develop their own desktop/web apps using Python.\n",
      "['also', 'home', 'due', 'covid', 'pandemic', 'created', 'personal', 'discount', 'spend', 'time', 'learning', 'new', 'higherpaying', 'job', 'skills', 'apply', 'discount', 'nanodegree', 'programs', 'udacitycom', 'jump', 'learning', 'new', 'skills', 'today', 'please', 'note', 'youll', 'need', 'sign', 'create', 'udacity', 'account', 'order', 'access', 'offer', 'premier', 'engineering', 'institute', 'indian', 'institute', 'technology', 'iit', 'kanpur', 'invited', 'applications', 'certification', 'course', 'python', 'online', 'course', 'accessed', 'free', 'july', 'coronavirus', 'pandemic', 'forced', 'universities', 'colleges', 'close', 'physical', 'classes', 'iit', 'kanpur', 'offering', 'online', 'course', 'python', 'programming', 'online', 'portal', 'students', 'enrol', 'course', 'access', 'free', 'till', 'july', 'st', 'even', 'opt', 'certification', 'paying', 'nominal', 'fee', 'taking', 'online', 'examination', 'python', 'programming', 'courses', 'covers', 'major', 'aspects', 'programming', 'language', 'designed', 'faculty', 'member', 'computer', 'science', 'engineering', 'prof', 'amey', 'karkare', 'curriculum', 'designed', 'beginners', 'assuming', 'student', 'prior', 'programming', 'knowledge', 'experience', 'course', 'covers', 'hours', 'online', 'lectures', 'since', 'prerecorded', 'session', 'students', 'access', 'lectures', 'per', 'convenience', 'syllabus', 'also', 'includes', 'source', 'code', 'files', 'handson', 'practice', 'students', 'use', 'reference', 'code', 'develop', 'desktopweb', 'apps', 'using', 'python']\n",
      "data\\uber.txt\n",
      "Our priority remains keeping our communities safe and healthy. In compliance with the Government Advisory to limit movement for reducing the spread of COVID-19, Uber services across India have been affected. This means that some or all of Uber services might not be available in your city. Please refer to the Uber page for the latest information on the status of Uber operations. This blog will be updated every few hours to ensure we are sharing all the necessary developments.\n",
      "\n",
      "We request you to please follow the instructions of the public health authorities and use Uber only if absolutely essential.\n",
      "\n",
      "Our hearts go out to everyone affected by COVID-19. We are taking extra precautions to keep our drivers and riders safe.\n",
      "['priority', 'remains', 'keeping', 'communities', 'safe', 'healthy', 'compliance', 'government', 'advisory', 'limit', 'movement', 'reducing', 'spread', 'covid', 'uber', 'services', 'across', 'india', 'affected', 'means', 'uber', 'services', 'might', 'available', 'city', 'please', 'refer', 'uber', 'page', 'latest', 'information', 'status', 'uber', 'operations', 'blog', 'updated', 'every', 'hours', 'ensure', 'sharing', 'necessary', 'developments', 'request', 'please', 'follow', 'instructions', 'public', 'health', 'authorities', 'use', 'uber', 'absolutely', 'essential', 'hearts', 'go', 'everyone', 'affected', 'covid', 'taking', 'extra', 'precautions', 'keep', 'drivers', 'riders', 'safe']\n",
      "unique_wordes {'news', 'machine', 'salesforce', 'months', 'curriculum', 'eventually', 'pandemic', 'recently', 'aspects', 'keeping', 'advisory', 'includes', 'days', 'spread', 'july', 'information', 'skills', 'pledging', 'created', 'developments', 'essential', 'create', 'would', 'apply', 'designed', 'companies', 'lead', 'applications', 'available', 'iit', 'latest', 'offer', 'minister', 'businesses', 'desktopweb', 'affect', 'source', 'reference', 'going', 'ensure', 'professional', 'close', 'compliance', 'extra', 'ceo', 'narendra', 'accessed', 'premier', 'colleges', 'jump', 'many', 'aspirant', 'karkare', 'files', 'amey', 'free', 'hearts', 'python', 'language', 'must', 'suffer', 'limit', 'health', 'forced', 'home', 'house', 'refer', 'also', 'programming', 'code', 'healthy', 'helping', 'data', 'operations', 'authorities', 'covers', 'access', 'safe', 'absolutely', 'marc', 'distancing', 'assuming', 'government', 'fight', 'st', 'drivers', 'remains', 'examination', 'handson', 'janata', 'cuts', 'online', 'portal', 'analytics', 'develop', 'workers', 'updated', 'request', 'janatacurfew', 'major', 'students', 'prime', 'engineering', 'youll', 'science', 'indian', 'sunday', 'staying', 'might', 'clarified', 'order', 'sign', 'go', 'reducing', 'participate', 'however', 'beginners', 'affected', 'urged', 'udacitycom', 'udacity', 'anywhere', 'convenience', 'means', 'practice', 'revenue', 'computer', 'support', 'three', 'opt', 'since', 'hack', 'modi', 'blog', 'aware', 'necessary', 'member', 'discount', 'apps', 'leadership', 'march', 'programs', 'new', 'significant', 'likely', 'nominal', 'technology', 'taking', 'vidhya', 'need', 'job', 'even', 'use', 'courses', 'stay', 'invited', 'competition', 'kanpur', 'per', 'hrs', 'across', 'twitter', 'ist', 'please', 'lectures', 'syllabus', 'account', 'note', 'knowledge', 'till', 'keep', 'prof', 'due', 'sharing', 'employees', 'pledged', 'personal', 'layoffs', 'today', 'offering', 'everyone', 'page', 'fee', 'loss', 'every', 'covid', 'published', 'using', 'experience', 'enrol', 'communities', 'social', 'services', 'honourable', 'physical', 'movement', 'status', 'invites', 'jobs', 'student', 'prerecorded', 'benioff', 'uber', 'curfew', 'nanodegree', 'course', 'certification', 'priority', 'announced', 'nd', 'faculty', 'hours', 'instructions', 'riders', 'firms', 'prior', 'initiative', 'good', 'statement', 'city', 'learning', 'institute', 'large', 'next', 'time', 'session', 'paying', 'public', 'senior', 'classes', 'follow', 'part', 'world', 'india', 'wholeheartedly', 'precautions', 'higherpaying', 'coronavirus', 'supports', 'universities', 'spend'}\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "dict_global = {}\n",
    "file_folder = 'data/*'\n",
    "\n",
    "idx = 1\n",
    "files_with_index = {}\n",
    "\n",
    "for file in glob.glob(file_folder):\n",
    "    print(file)\n",
    "    fname = file\n",
    "    file = open(file, 'r')\n",
    "    text = file.read()\n",
    "    print(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = re.sub(re.compile('\\d'), '', text)\n",
    "    \n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in Stopwords]\n",
    "    print(words)\n",
    "    \n",
    "    dict_global.update(finding_all_unique_words_and_freq(words))\n",
    "    \n",
    "    \n",
    "    files_with_index[idx] = os.path.basename(fname)\n",
    "    idx = idx + 1\n",
    "    \n",
    "unique_words_all = set(dict_global.keys())\n",
    "print('unique_wordes', unique_words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, docId, freq= None):\n",
    "        self.freq = freq\n",
    "        self.doc = docId\n",
    "        self.nextval = None\n",
    "        \n",
    "class SlinkedList:\n",
    "    def __init__(self, head= None):\n",
    "        self.head = head\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Node object at 0x0000022D1FD5D208>\n",
      "<__main__.Node object at 0x0000022D1FD5EC48>\n",
      "<__main__.Node object at 0x0000022D1FD5D988>\n"
     ]
    }
   ],
   "source": [
    "linked_list_data = {}\n",
    "for word in unique_words_all:\n",
    "    linked_list_data[word] = SlinkedList()\n",
    "    linked_list_data[word].head = Node(1,Node)\n",
    "word_freq_in_doc = {}\n",
    "idx = 1\n",
    "for file in glob.glob(file_folder):\n",
    "    file = open(file, \"r\")\n",
    "    text = file.read()\n",
    "    text = remove_special_characters(text)\n",
    "    text = re.sub(re.compile('\\d'),'',text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if len(words)>1]\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in Stopwords]\n",
    "    word_freq_in_doc = finding_all_unique_words_and_freq(words)\n",
    "    for word in word_freq_in_doc.keys():\n",
    "        linked_list = linked_list_data[word].head\n",
    "        while linked_list.nextval is not None:\n",
    "            linked_list = linked_list.nextval\n",
    "        linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n",
    "    print(linked_list)\n",
    "    idx = idx + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your querymodi and covid\n",
      "['and']\n",
      "['modi', 'covid']\n",
      "modi\n",
      "covid\n",
      "[1, 0, 0] [1, 1, 1]\n",
      "[[1, 0, 0], [1, 0, 0]]\n",
      "[[1, 0, 0], [1, 0, 0]]\n",
      "['a_v.txt']\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter your query')\n",
    "query = word_tokenize(query)\n",
    "\n",
    "connecting_words =[]\n",
    "cnt = 1\n",
    "\n",
    "different_words = []\n",
    "\n",
    "for word in query:\n",
    "    if word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n",
    "        different_words.append(word.lower())\n",
    "    else:\n",
    "        connecting_words.append(word.lower())\n",
    "print(connecting_words)\n",
    "print(different_words)\n",
    "\n",
    "total_files = len(files_with_index)\n",
    "\n",
    "zeroes_and_ones = []\n",
    "zeroes_and_ones_of_all_words = []\n",
    "\n",
    "for word in different_words:\n",
    "    if word.lower() in unique_words_all:\n",
    "        zeroes_and_ones = [0]*total_files\n",
    "        linked_list = linked_list_data[word].head\n",
    "        print(word)\n",
    "        while linked_list.nextval is not None:\n",
    "            zeroes_and_ones[linked_list.nextval.doc - 1] = 1\n",
    "            linked_list = linked_list.nextval\n",
    "        zeroes_and_ones_of_all_words.append(zeroes_and_ones)\n",
    "    else:\n",
    "        print(word,\" not found\")\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "for word in connecting_words:\n",
    "    word_list1 = zeroes_and_ones_of_all_words[0]\n",
    "    word_list2 = zeroes_and_ones_of_all_words[1]\n",
    "    \n",
    "    print(word_list1, word_list2)\n",
    "    if word == 'and':\n",
    "        bitwise_op = [w1 & w2 for (w1, w2) in zip(word_list1, word_list2)]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.insert(0, bitwise_op)\n",
    "        \n",
    "    elif word == 'or':\n",
    "        \n",
    "        \n",
    "        bitwise_op = [w1 | w2 for(w1, w2) in zip(word_list1, word_list2)]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "        \n",
    "    elif word == 'not':\n",
    "        bitwise_op = [not w1 for w1 in word_list2]\n",
    "        bitwise_op = [int(b == True) for b in bitwise_op]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,bitwise_op)]\n",
    "        \n",
    "zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "print(zeroes_and_ones_of_all_words)\n",
    "\n",
    "        \n",
    "files = []    \n",
    "print(zeroes_and_ones_of_all_words)\n",
    "lis = zeroes_and_ones_of_all_words[0]\n",
    "cnt = 1\n",
    "for index in lis:\n",
    "    \n",
    "    if index == 1:\n",
    "        files.append(files_with_index[cnt])\n",
    "    cnt = cnt+1\n",
    "    \n",
    "print(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
